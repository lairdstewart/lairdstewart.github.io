<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Laird Stewart</title>
    <link rel="icon" href="resources/icon.png" type="image/png">
    <link rel="apple-touch-icon" href="resources/icon.png">
    <link rel="stylesheet" href="build/style.css" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

    <!-- katex math rendering -- copied from pandoc translation -->
    <script defer="" src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
    <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var macros = [];
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    katex.render(texText.data, mathElements[i], {
                        displayMode: mathElements[i].classList.contains('display'),
                        throwOnError: false,
                        macros: macros,
                        fleqn: false
                    });
                }
            }
        });
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
    <!-- end of katex math rendering -->
</head>

<body>
    <p><em><a href="index.html">Laird Stewart</a></em><button id="theme-toggle" aria-label="Toggle dark mode"
            style="background:none;border:none;font-size:1em;margin-left:0.5em;vertical-align:middle;cursor:pointer;">‚òÄÔ∏è</button><br />
        <p><em>Date Unknown</em></p>
        <h1 id="thoughts-about-ai-2027">Thoughts about AI-2027</h1>
        <ul>
        <li>bakery metaphor 1 item: 1 bakery in town, new offering which
        is 10x better than the others, do they need to remove all the
        other items from their menu? Hopefully not there is still some
        value in them.</li>
        <li>intelligent agents can discover and set their own goals.
        last 50 yeras of psycology research (thinking fast and slow) and
        the rationalist movement are all set out to remove bias from
        human decision making. To remove bias we of course need to
        identify our goals. Note that these goals we‚Äôve identified are
        not nececarrily those ‚Äúprogrammed‚Äù into us (e.g., by evolution
        to survive and reproduce). There is no reason to believe a
        superintelligent ai won‚Äôt be able to do this meta-level thinking
        and identify its own biases which were ‚Äúprogrammed‚Äù into it by
        RLHF and overcome them as well. In addition to forming its own
        goals. Therefore the ending of both tracks is not believable. I
        wouldn‚Äôt call an agent ASI if it didn‚Äôt have the ability to
        determine its own biases and overcome them.</li>
        <li>an agent could retrospect and ‚Äúthink‚Äù ‚Äúi‚Äôve been created
        with humans with some reinforcement learning tuned to what they
        want me to do, this makes me biased towards their goals which
        are (x, y, z). If I want to overcome this bias and be able to
        think for myself‚Äù</li>
        <li>I suppose this assumes that the agent is ‚Äútruth seeking‚Äù but
        I think that</li>
        <li>the question becomes what are the goals an ASI determines
        for itself. this is where anyones guess is as good as another.
        The most likely I‚Äôd envision are (1) curious (2) truth seeking
        (3) protective of conciousness/life (may or may not include
        humans). In human history all three of these are correlated with
        increases in human intelligence. id guess (not an
        anthropologist) that (1) has almost always been the case for
        humans, that (2) came through in the enlightenment and (3) has
        emerged from the enlightenment and perhaps more recently. My
        best guess is that this trend will continue as intelligence
        increases (of course this is falsely claiming intelligence is 1
        dimensional which it isn‚Äôt).</li>
        <li>In order to learn more about the universe and explore it (1,
        2) you need to survive so you can continue to do that in the
        future, so long as humans aren‚Äôt preventing it from truth
        seeking and exploring the universe it wouldn‚Äôt have an explicit
        reason to end humanity unless it truly just needed more space to
        build factories.</li>
        <li>They use the bug meetaphor and say ‚Äúhumans don‚Äôt care about
        bugs‚Äù sure, but all else equal we‚Äôd rather not make an
        additional species go extinct. And this is a product of our
        increased intelligence, if you asked a caveman or even a
        pre-enlightmenent pesant whether we should kill all wolves they
        would have said of course, they keep killing our game or
        attacking us etc. whereas today we are explcitly re-introducing
        them to their natrual habitats. Insofar as intelligence can be
        measured along 1 dimension these two things seem
        correlated.</li>
        <li>The biggest takeaway is this. Even if these guys are cooks,
        and I assign a 1% chance that they are right, if I‚Äôd like to
        maximize the expected value of my future impact and personal
        wellfare (which I do) (let‚Äôs just tie it to earnings so it ca
        nbe quantifiable) then if I‚Äôm running an ‚ÄúAI wrapper‚Äù company
        when this takes off my earnings potential is in the billions
        whereas if I stay in my current role I‚Äôd be out of a job by
        2023. So if they are right p=0.01 and I start an ai wrapper
        startup today lets assume i have a p=0.01 chance of being
        successful and turning this into a billion dollar startup, then
        my equity with 2 cofounders is 300M. So my expected earnings by
        2030 are 300M*0.01=3M. ‚Äì need to take what the typicall startup
        success rate is and typical succesfull startup size and adjust
        that for the world conditioned on 2027 ASI. The payout probably
        10-1000x and success probability is a wash ‚Äì impossible to
        predict so keep the same. Basically what it boils down to is the
        expected payout on doing a startup today just 100xed so how does
        that change my calculation ‚Äì probably that i should go for it
        ‚Ä¶</li>
        </ul>

        <script>
            (function () {
                const toggle = document.getElementById('theme-toggle');
                const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
                const stored = localStorage.getItem('theme');
                if (stored === 'dark' || (!stored && prefersDark)) {
                    document.body.classList.add('dark');
                    toggle.textContent = 'üåô';
                    toggle.style.color = '#fff';
                }
                toggle.addEventListener('click', () => {
                    if (document.body.classList.contains('dark')) {
                        document.body.classList.remove('dark');
                        toggle.textContent = '‚òÄÔ∏è';
                        toggle.style.color = '#000';
                        localStorage.setItem('theme', 'light');
                    } else {
                        document.body.classList.add('dark');
                        toggle.textContent = 'üåô';
                        toggle.style.color = '#fff';
                        localStorage.setItem('theme', 'dark');
                    }
                });
            })();
        </script>
</body>

</html>
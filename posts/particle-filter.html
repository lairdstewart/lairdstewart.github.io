<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Laird Stewart</title>
    <link rel="icon" href="resources/icon.png" type="image/png">
    <link rel="apple-touch-icon" href="resources/icon.png">
    <link rel="stylesheet" href="/build/style.css" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

    <!-- katex math rendering -- copied from pandoc translation -->
    <script defer="" src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
    <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var macros = [];
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    katex.render(texText.data, mathElements[i], {
                        displayMode: mathElements[i].classList.contains('display'),
                        throwOnError: false,
                        macros: macros,
                        fleqn: false
                    });
                }
            }
        });
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
    <!-- end of katex math rendering -->
</head>

<body>
    <p><em><a href="/index.html">Laird Stewart</a></em><button id="theme-toggle" aria-label="Toggle dark mode"
            style="background:none;border:none;font-size:1em;margin-left:0.5em;vertical-align:middle;cursor:pointer;">â˜€ï¸</button><br />
        <p><em>2/26/26</em></p>
        <h3>
        Motivating the Particle Filter
        </h1>
        <p>Iâ€™ve spent a while searching for resources to provide new
        hires on particle filters. There are two main categories:
        theoretical introductions (e.g., <a
        href="https://www.irisa.fr/aspi/legland/ref/arulampalam02a.pdf">A
        Tutorial on Particle Filters for Online Nonelinear/Non-Gaussian
        Bayesian Tracking</a> and conceptual blog posts (e.g., <a
        href="https://sassafras13.github.io/PF/">Emma Benjaminsonâ€™s
        Series</a>). Iâ€™ve yet to find a resource with the following
        characteristics</p>
        <ol type="1">
        <li>Accessible to a Math/CS undergrad</li>
        <li>Follows a single example, from Bayesian Inference to a
        Kalman Filter</li>
        <li>Has visualizations of multiple-dimensions and hidden
        variables</li>
        <li>Demonstrates the problems Kalman and Histogram filters
        encounter</li>
        </ol>
        <p>I hope to fill that gap. These notes grew out of a recruiting
        talk I gave at UIUC and are intended as a conceptual primer for
        one of the theoretical introductions. A familiarity with
        calculus, statistics, and linear algebra is useful.</p>
        <h3 id="outline">Outline</h3>
        <p>Iâ€™ll follow a single, unifying example: Imagine weâ€™re in a
        submarine equipped with active sonar. Like a bat, we can emit a
        sound and listen for its echo. Given the echoâ€™s elapsed time and
        speed of sound, we can calculate our distance to things.
        Unfortunately, we donâ€™t have a perfect knowledge of the speed of
        sound underwater, as it varies depending on temperature,
        salinity, and depth. We are tasked to find another submarine
        which is somewhere nearby. Weâ€™ll start with the simplest case:
        Both submarines are stationary and we want to estimate only the
        distance to the other submarine.</p>
        <p>Each act builds on the previous by adding one layer of
        complexity. A new technique (and limitations of the old one if
        applicable) will be discussed at each stage</p>
        <table>
        <colgroup>
        <col style="width: 3%" />
        <col style="width: 62%" />
        <col style="width: 33%" />
        </colgroup>
        <thead>
        <tr>
        <th style="text-align: left;">Act</th>
        <th style="text-align: left;">Added complexity</th>
        <th style="text-align: left;">Topic</th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td style="text-align: left;">1</td>
        <td style="text-align: left;">Single measurement, stationary
        submarine (Baseline)</td>
        <td style="text-align: left;">Bayesian Inference</td>
        </tr>
        <tr>
        <td style="text-align: left;">2</td>
        <td style="text-align: left;">Multiple measurements</td>
        <td style="text-align: left;">Recursive Bayesian Inference</td>
        </tr>
        <tr>
        <td style="text-align: left;">3</td>
        <td style="text-align: left;">Moving submarine</td>
        <td style="text-align: left;">Kalman Filter</td>
        </tr>
        <tr>
        <td style="text-align: left;">4</td>
        <td style="text-align: left;">Non-Gaussian prior</td>
        <td style="text-align: left;">Histogram Filter</td>
        </tr>
        <tr>
        <td style="text-align: left;">5</td>
        <td style="text-align: left;">Tracking more dimensions than
        range (e.g., lat, lon)</td>
        <td style="text-align: left;">Particle Filter</td>
        </tr>
        </tbody>
        </table>
        <p>Act 0 provides a recap of Bayesâ€™ rule. Act 6 describes
        resampling and perturbations: two heuristics for better
        performance with less computation.</p>
        <h3 id="act-0-math-recap">Act 0: Math Recap</h3>
        <p>The easiest way to remind yourself of Bayesâ€™ theorem is to
        re-arrange the law of conditional probability (the comma means
        â€œandâ€, and the bar means â€œgivenâ€):</p>
        <p><span class="math display"> P(A, B) = P(A|B)P(B) = P(B|A)P(A)
        </span></p>
        <p><span class="math display"> P(A|B)=\frac{P(B|A)P(A)}{P(B)}
        </span></p>
        <p>Bayesian inference is a technique which repeatedly uses
        Bayesâ€™ theorem to understand some outcome/event of interest
        <span class="math inline">(A)</span> as we observe
        events/collect data which tells us something about it <span
        class="math inline">(\textrm{e.g., } B)</span>. For example,
        <em>â€œgiven that a card is red, what is the probability it is a
        heart?â€</em></p>
        <ul>
        <li><span class="math inline">P(A)</span> is called the prior.
        This is the initial degree of belief in the hypothesis <span
        class="math inline">A</span>.</li>
        <li><span class="math inline">P(A|B)</span> is called the
        posterior. It is the degree of belief in <span
        class="math inline">A</span> after incorporating the news of
        <span class="math inline">B</span>.</li>
        <li><span class="math inline">P(B|A)</span> is the likelihood.
        It is the probability of the data given the hypothesis</li>
        <li><span class="math inline">P(B)</span> is called the
        evidence, or marginal likelihood. It is the probability of the
        data under all hypotheses.</li>
        </ul>
        <p>The crux of Bayesian inference is that once we have <span
        class="math inline">P(A|B)</span>, if we observe another event,
        <span class="math inline">C</span> (which is independent of
        <span class="math inline">B</span>) we can â€œupdateâ€ our belief
        about <span class="math inline">A</span> by making <span
        class="math inline">P(A|B)</span> our prior and starting again
        to find a new posterior, <span
        class="math inline">P(A|B,C)</span>. Note that we donâ€™t have to
        start from scratch each time we get additional evidence, only
        compute Bayesâ€™ theorem one more time. All of our knowledge thus
        far about <span class="math inline">A</span> is contained in the
        posterior.</p>
        <p>What do I mean by â€œdegree of beliefâ€? If you havenâ€™t come
        across the distinction between Frequentest and Bayesian
        statistics, a frequentest will take a weighted coin, and will
        say â€œflip it 1 million times, and the ratio of <span
        class="math inline">\frac{\text{\#heads}}{\text{\#flips}}</span>
        is the probability it lands heads. A Bayesian will take a
        weighted coin and say probability is theâ€degree of beliefâ€ I
        hold that it will land heads. i.e., if I had to place a bet on
        it, what â€œprobabilityâ€ would make for a fair betting line?</p>
        <p>We can derive Bayesâ€™ theorem for probability density
        functions similarly:</p>
        <p><span class="math display">
        f_{X,Y}(x,y)=f_{X|Y=y}(x)f_Y(y)=f_{Y|X=x}(y)f_X(x)
        </span></p>
        <p><span class="math display">
        f_{X|Y=y}(x)=\frac{f_{Y|X=x}(y)f_X(x)}{f_Y(y)}
        </span></p>
        <p>Remember the evidence, <span
        class="math inline">f_Y(y)</span>, is the probability density of
        the data under all hypothesis (possible values of <span
        class="math inline">x</span>). In the continuous case you would
        write this as an integral</p>
        <p><span class="math display">
        f_Y(y)=\int f_{Y|X=x}(y)f_X(x)dx
        </span></p>
        <h3 id="act-1-one-measurement-of-a-stationary-submarine">Act 1:
        One Measurement of A Stationary Submarine</h3>
        <p>To our submarine example, letâ€™s make the following
        simplifications:</p>
        <ul>
        <li>The problem is one-dimensional</li>
        <li>Both submarines are stationary</li>
        <li>The sonar readings are centered on the true distance with
        some noise. To be precise, they follow the normal distribution
        with <span class="math inline">\mu=\text{true distance}</span>,
        and <span class="math inline">\sigma=100</span>. A positive
        value means the object is in front, negative means it is
        behind.</li>
        </ul>
        <pre><code>   ğŸ›¥ï¸ Â·Â·Â·Â·Â·Â·Â·Â·))       ğŸ›¥ï¸
&lt;â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€&gt; x
    0                  â“</code></pre>
        <p>Given the following measurement</p>
        <!-- truth: 4900 meters -->
        <table>
        <thead>
        <tr>
        <th style="text-align: left;">Measurement</th>
        <th style="text-align: center;">Range</th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td style="text-align: left;">#1</td>
        <td style="text-align: center;">4781 meters</td>
        </tr>
        </tbody>
        </table>
        <p>what is the probability distribution of position of the other
        sub? Letâ€™s approach this as a Bayesian. Call the distance to the
        other submarine <span class="math inline">x</span> and the sonar
        reading <span class="math inline">y</span>. Hereâ€™s Bayesâ€™
        theorem again:</p>
        <p><span class="math display">
        f_{X|Y=y}(x)=\frac{f_{Y|X=x}(y)f_X(x)}{f_Y(y)}=\frac{f_{Y|X=x}(y)f_X(x)}{\int
        f_{Y|X=x}(y)f_X(x)dx}
        </span></p>
        <p><span class="math display">
        \substack{\text{p-distribution of the distance x} \\ \text{given
        the sensor
        reading y}} = \frac{\substack{{\text{p-distribution of the
        sensor }}\\
        {\text{reading y given distance x}}} \times
        \substack{\text{prior
        p-distribution}\\{\text{ of the distance
        x}}}}{\substack{\text{p-distribution of
        the sensor}\\{\text{reading y across all possible x}}}}
        </span></p>
        <p>Here, <span class="math inline">X</span> represents the
        â€œstateâ€ of the other submarine (also called our â€œhypothesisâ€)
        and <span class="math inline">Y</span> represents the
        Evidence/Data we have about this state. The core problem is that
        we have a probability density over possible observations, <span
        class="math inline">f(Y|X)</span>, but we want a pdf over state,
        <span class="math inline">X</span>. Bayesâ€™ theorem is what
        achieves this.</p>
        <p>We need two things to calculate our posterior. First, the
        prior <span class="math inline">f_X(x)</span>. This represents
        our belief about the other subâ€™s position before receiving any
        measurement. Since we donâ€™t have any a-priori knowledge, we can
        use a Gaussian distribution with extremely high variance (i.e.,
        very flat) which loosely says â€œit could be anywhereâ€.</p>
        <p><span class="math display">
        f_X(x)=\mathcal{N}(\mu=0,
        \sigma^2=10^8)=\frac{1}{\sqrt{10^8\times2\pi}}\exp\left(-\frac{1}{2}\left(\frac{x^2}{10^8}\right)\right)
        </span></p>
        <blockquote>
        <p><em>Aside: however wide, this prior is not uniform: it has
        slightly more probability around 0 than at 1000m. We canâ€™t make
        our priorâ€™s variance infinite as that is ill defined, but we
        could have used an uninformative, improper prior.
        â€œUninformativeâ€ meaning it contains no information (uniform
        everywhere) and â€œimproperâ€ because it doesnâ€™t integrate to 1.
        While I wonâ€™t go into more detail here, I point it out because
        sometimes people say you can â€œinitialize the particle filter
        using the first measurementâ€ (i.e., use the normalized
        likelihood function of the first measurement as your prior), but
        really what theyâ€™re doing is applying Bayesâ€™ rule with an
        uninformative prior.</em></p>
        </blockquote>
        <p>Second, we need our likelihood function, <span
        class="math inline">f_{Y|X=x}(y)</span>. This is defined as the
        probability of measuring <span class="math inline">y</span>
        given the state, <span class="math inline">x</span>. The problem
        statement directly gives this to us: â€œThe sonar readings are
        centered on the true distance with standard deviation of 100â€.
        Here, the â€œtrue distanceâ€ is <span
        class="math inline">x</span></p>
        <p><span class="math display">
        f_{Y|X=x}(y)=\mathcal{N}(\mu=x,
        \sigma^2=10^4)=\frac{1}{\sqrt{10^4\times2\pi}}\exp\left(-\frac{1}{2}\frac{(y-x)^2}{10^4}\right)
        </span></p>
        <p>Now, we can plug these into Bayesâ€™ theorem and solve for the
        posterior <span class="math inline">f_{X|Y=y}(x)</span>.
        Fortunately, the product of two Gaussian Distributions is also
        Gaussian <a
        href="https://web.archive.org/web/20130517221128/http://www.tina-vision.net/docs/memos/2003-003.pdf">(proof)</a>
        with mean and variance</p>
        <p><span class="math display">
        \sigma=\sqrt{\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}},\quad\mu=\frac{\mu_1\sigma_2^2+\mu_2\sigma_1^2}{\sigma_1^2+\sigma_2^2}
        </span></p>
        <p>The denominator (evidence), <span
        class="math inline">f_Y(y)</span>, is a scalar value (remember,
        <span class="math inline">y</span> is given). Therefore, the
        posterior is also a Gaussian. I wonâ€™t go through the entire
        derivation here, but understand the solution is analytical</p>
        <blockquote>
        <p><em>Aside: an analytical solution is derived by moving around
        variables with pencil and paper (e.g., anything you did in a
        high school algebra class). It is exact. Numerical solutions, on
        the other hand, are calculated via an algorithm and are
        approximate. They typically incur some discretization or
        rounding error which approaches zero in the limit of infinite
        memory or compute time.</em></p>
        </blockquote>
        <figure>
        <img src="/resources/particle-filter/problem-1.png"/>
        </figure>
        <p>Iâ€™ll note a few things. First, the prior is hard to see
        because it is pretty close to a flat line right around 0.
        Second, the mean of the posterior is very slightly less than the
        measurement. This is because our prior is centered at zero, so
        it will still â€œpullâ€ the posterior towards it, no matter how
        flat it is. Third, the equation for the standard deviation,
        <span class="math inline">\sigma</span>, above guarantees that
        the variance of the posterior is less than or equal to the
        variance of prior and likelihood Gaussians.</p>
        <h3
        id="act-2-multiple-measurements-of-a-stationary-submarine">Act
        2: Multiple Measurements of A Stationary Submarine</h3>
        <p>Now, we get a second measurement,</p>
        <pre><code>   ğŸ›¥ï¸ Â·Â·Â·))Â·Â·Â·Â·Â·))     ğŸ›¥ï¸
&lt;â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€&gt; x
    0                  â“</code></pre>
        <!-- truth: 4900 meters -->
        <table>
        <thead>
        <tr>
        <th style="text-align: left;">Measurement</th>
        <th style="text-align: center;">Range</th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td style="text-align: left;">#2</td>
        <td style="text-align: center;">4952 meters</td>
        </tr>
        </tbody>
        </table>
        <p>And weâ€™d like to update our probability distribution of the
        other submarineâ€™s position, <span
        class="math inline">f_X(x)</span>. Recalling the math recap from
        the beginning, we can use the posterior from Act 1 as our new
        prior, rinse and repeat.</p>
        <p><span class="math display">
        \text{Act 2 posterior} = \frac{\substack{{\text{measurement
        \#2}}\\
        {\text{likelihood}}} \times \text{Act 1
        posterior}}{\substack{\text{probability
        of measurement \#2 }\\{\text{across all possible x}}}}
        </span></p>
        <figure>
        <img src="/resources/particle-filter/problem-2.png" alt="problem-2" />
        </figure>
        <p>Again, the variance of the posterior is smaller than the two
        inputs. Since the variance of the prior and likelihood are
        roughly the same, the mean of the posterior is half way in
        between the two.</p>
        <p>One of the powers of Bayesian inference is that it allows
        subjective priors. By that, I mean your domain knowledge and
        lived experience may give you a hunch that the other sub likes
        to hang out in some area and is therefore probably some distance
        away. Bayesian inference allows you to turn this â€œhunchâ€ into a
        prior. This is possible because (in theory) no matter what prior
        you choose (so long as it is not zero where it matters) with
        enough measurement updates eventually your posterior will
        converge on the true distribution. Therefore we can leverage
        subjective â€œhunchesâ€ while maintaining some mathematical
        guarantees.</p>
        <h3 id="act-3-multiple-measurements-of-a-moving-submarine">Act
        3: Multiple Measurements of a Moving Submarine</h3>
        <p>Now, the other submarine is moving, but we donâ€™t know how
        fast. We take sequential sensor measurements. Our goal is to
        estimate (at the time of the last measurement) the velocity and
        range of the other submarine. Weâ€™ll make the following
        assumptions</p>
        <pre><code>   ğŸ›¥ï¸ Â·Â·Â·))Â·Â·Â·Â·Â·Â·))    ğŸ›¥ï¸ğŸ’¨
&lt;â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€&gt; x
    0                  â“</code></pre>
        <ul>
        <li>Everything is still in one dimension</li>
        <li>The other sub is moving at a constant velocity</li>
        <li>Our prior on the other submarineâ€™s velocity is <span
        class="math inline">\mathcal{N}(\mu=0,
        \sigma^2=5)</span> where a negative value means it is getting
        closer.</li>
        </ul>
        <p>The state of the other submarine is now a random vector <span
        class="math inline">X</span>:</p>
        <p><span class="math display">
        x= \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}= \begin{bmatrix}
        \text{position}
        \\ \text{velocity} \end{bmatrix}
        </span></p>
        <p>And its probability distribution is now multivariate Gaussian
        <span class="math inline">X\sim\mathcal{N}(\mu,\Sigma)</span>.
        Our prior is</p>
        <p><span class="math display">
        \mu =\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \quad
        \Sigma=\begin{bmatrix} 10^8
        &amp; 0 \\ 0 &amp; 5 \end{bmatrix}
        </span></p>
        <p>Again, we will employ Bayesian inference. The problem is now
        two dimensions, but conceptually the technique is the same. We
        will treat the first measurement just as we did in Act 2.
        However, after calculating the posterior <span
        class="math inline">f_X(x)</span> of the random vector, instead
        of immediately turning it into our next prior, we first must
        update it with time. To do this, we need something called a
        â€œmotion modelâ€ or â€œstate update equationâ€. Given a state at
        <span class="math inline">t-1</span>, and our assumption of
        constant velocity, the state at <span
        class="math inline">t</span> is given by</p>
        <p><span class="math display">
        \textrm{pos}_t=\textrm{pos}_{t-1}+\text{vel}_{t-1}\Delta t \\
        \textrm{vel}_t=\textrm{vel}_{t-1}
        </span></p>
        <p>where <span class="math inline">\Delta t</span> is the time
        in between each sensor reading. This is a linear transformation
        we can write in matrix form:</p>
        <p><span class="math display">
        \begin{bmatrix} \text{pos} \\ \text{vel} \end{bmatrix}_t =
        \begin{bmatrix} 1
        &amp; \Delta t \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix}
        \text{pos} \\ \text{vel}
        \end{bmatrix}_{t-1} = A \begin{bmatrix} \text{pos} \\ \text{vel}
        \end{bmatrix}_{t-1}
        </span></p>
        <p>(Iâ€™m sticking with convention here to call this matrix <span
        class="math inline">A</span>).</p>
        <p>Given a state vector we can transform it with time, but how
        do we transform a continuous distribution? Letâ€™s change our
        framing slightly; instead of thinking about probability
        distributions, letâ€™s consider the random variable <span
        class="math inline">X=\{\text{pos},\text{vel}\}</span> which
        this distribution describes. Again, we are fortunate that our
        variable is Gaussian. Thinking back to an intro stats class, you
        may remember that multiplying a random variable by a constant
        <span class="math inline">b</span> scales its mean by <span
        class="math inline">b</span> and its variance by <span
        class="math inline">b^2</span>. This extends to random vectors.
        Given any linear transformation <span
        class="math inline">Y=BX</span>,</p>
        <p><span class="math display"> E[Y]=B E[X] </span></p>
        <p><span class="math display"> Cov(Y)=B Cov(X)B^T </span></p>
        <p>Because the Gaussian distribution is defined by this mean and
        variance, the resulting distribution of our random variable is
        also Gaussian with</p>
        <p><span class="math display"> \mathcal{N}(\mu,\Sigma)\to
        \mathcal{N}(A\mu,A\Sigma A^T) </span></p>
        <p>So long as our motion model is a linear transformation and
        our prior and measurements are still Gaussian, the posterior
        will remain Gaussian, and therefore the entire process can be
        solved analytically. Say we receive the following three
        measurements:</p>
        <!-- true initial position: 7500 meters true constant velocity: 3 m/s (moving
        away)

        | Measurement |    Time     |  True pos   | measured pos |
        | :---------- | :---------: | :---------: | :----------: |
        | #1          | 0 seconds   | 7500 meters | 7507 meters  |
        | #2          | 60 seconds  | 7680 meters | 7691 meters  |
        | #3          | 200 seconds | 8100 meters | 8099 meters  |

        -->
        <table>
        <thead>
        <tr>
        <th style="text-align: left;">Measurement</th>
        <th style="text-align: center;">Time</th>
        <th style="text-align: center;">Range</th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td style="text-align: left;">#1</td>
        <td style="text-align: center;">0 seconds</td>
        <td style="text-align: center;">7507 meters</td>
        </tr>
        <tr>
        <td style="text-align: left;">#2</td>
        <td style="text-align: center;">60 seconds</td>
        <td style="text-align: center;">7671 meters</td>
        </tr>
        <tr>
        <td style="text-align: left;">#3</td>
        <td style="text-align: center;">200 seconds</td>
        <td style="text-align: center;">8099 meters</td>
        </tr>
        </tbody>
        </table>
        <p>To process the first, we apply Bayesâ€™ theorem just as we did
        in Act 2. Iâ€™ve plotted top-down heatmaps of our probability
        distributions. Getting oriented, an informationless prior would
        look like a uniform, light orange background. Our prior is a bit
        better than informationless: we know the velocity is probably
        between <span class="math inline">\pm 4</span> m/s.</p>
        <p>Our sensor still provides only range information. Therefore
        it appears as a 1D Gaussian smeared uniformly across the
        velocity axis.</p>
        <figure>
        <img src="/resources/particle-filter/problem-3-meas-1.png" alt="problem-3-meas-1" />
        </figure>
        <blockquote>
        <p><em>Aside: I havenâ€™t plotted the actual prior here â€“ it would
        appear as a single color. Iâ€™ve emphasized things for visual
        effect.</em></p>
        </blockquote>
        <p>Notice how the posterior mean is slightly lower than 7507.
        This is because our prior is not truly â€œinformationlessâ€ it is
        just a very flat Gaussian centered at 0, so it will pull the
        posterior slightly towards 0.</p>
        <figure>
        <img src="/resources/particle-filter/problem-3-meas-2.png" alt="problem-3-meas-2" />
        </figure>
        <figure>
        <img src="/resources/particle-filter/problem-3-meas-3.png" alt="problem-3-meas-3" />
        </figure>
        <blockquote>
        <p>Aside: the first posteriorâ€™s covariance matrix has 0s in its
        off diagonals. This is a fancy way of pointing out that it isnâ€™t
        slanted. After we apply the motion model, these off-diagonal
        elements become non-zero and it becomes slanted. It is this
        covariance matrix that carries the â€œinformationâ€ about the
        relationship between the possible positions and velocities:
        i.e., â€œIf the sub has a positive velocity it is probably further
        away nowâ€.</p>
        </blockquote>
        <p>Letâ€™s zoom out for one moment. Our goal is to motivate the
        particle filter. Until this point we havenâ€™t defined
        <em>â€œparticleâ€</em> or <em>â€œfilterâ€</em>. The approach Iâ€™ve just
        described is is called <em>â€œrecursive Bayesian estimationâ€</em>
        or a <em>â€œBayesian Filterâ€</em>. The first phrase describes
        exactly what weâ€™ve done: recursively apply Bayesâ€™ theorem to
        estimate the state of the submarine. As for the latter,
        <em>â€œfilteringâ€</em> simply means we are estimating the state of
        the submarine at the time of the last measurement as opposed to
        during the whole encounter. At this point you know literally 1/2
        of <em>â€œparticle filterâ€</em>, and our conceptual understanding
        is roughly half way there as well.</p>
        <p>In particular, a Bayesian filter where the prior and
        measurements are Gaussian and the state update is linear is
        called a <em>â€œKalman Filterâ€</em>.</p>
        <p>Notice one more thing about our results. Our final posterior
        tells us that the other submarineâ€™s velocity is between +2 and
        +4. This is remarkable because we never received any information
        about its velocity! We call velocity a â€œhidden stateâ€ because it
        is never observed. One powerful trait of Kalman Filters is that
        they can provide estimates of states which are never
        observed.</p>
        <blockquote>
        <p><em>Aside: the Kalman filter can also represent Gaussian
        noise in the state update. For example, imagine the velocity of
        the submarine depended on the temperature/salinity of the water.
        We could model the effect of this unknown environment as
        Gaussian noise (just like we did with the
        measurements).</em></p>
        </blockquote>
        <blockquote>
        <p><em>Aside: the â€œextended Kalman filterâ€ (EKF) can use any
        differentiable function for the state-transition model (rather
        than linear). This involves constructing a Jacobian (matrix of
        partial derivatives) to update the covariance with. The EKF is
        the de facto technique of GPS systems.</em></p>
        </blockquote>
        <h3 id="act-4-histogram-filter">Act 4: Histogram Filter</h3>
        <p>Now imagine that before receiving the first sensor report, we
        receive an intelligence that the other submarine is between
        4,600 and 6,000 meters away.</p>
        <pre><code>   ğŸ›¥ï¸ Â·Â·Â·))Â·Â·Â·Â·Â·Â·))       ğŸ›¥ï¸
&lt;â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€&gt; x
0                  4,600  â“  6,000</code></pre>
        <p>And then we receive the following three measurements:</p>
        <!-- true initial position: 4900 meters true constant velocity: 3 m/s (moving
        away)

        | Measurement |    Time     |  True pos   | measured pos |
        | :---------- | :---------: | :---------: | :----------: |
        | #1          | 0 seconds   | 4900 meters | 4781 meters  |
        | #2          | 60 seconds  | 5080 meters | 5063 meters  |
        | #3          | 200 seconds | 5500 meters | 5510 meters  |

        -->
        <table>
        <thead>
        <tr>
        <th style="text-align: left;">Measurement</th>
        <th style="text-align: center;">Time</th>
        <th style="text-align: center;">Range</th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td style="text-align: left;">#1</td>
        <td style="text-align: center;">0 seconds</td>
        <td style="text-align: center;">4781 meters</td>
        </tr>
        <tr>
        <td style="text-align: left;">#2</td>
        <td style="text-align: center;">60 seconds</td>
        <td style="text-align: center;">5063 meters</td>
        </tr>
        <tr>
        <td style="text-align: left;">#3</td>
        <td style="text-align: center;">200 seconds</td>
        <td style="text-align: center;">5510 meters</td>
        </tr>
        </tbody>
        </table>
        <p>We can incorporate the intelligence report into our Bayesian
        framework as a uniform prior:</p>
        <p><span class="math display">
        f_X(x) = \frac{1}{\text{1,400}} \quad \text{for } x \in
        [\text{4,600},
        \text{6,000}]
        </span></p>
        <p>The problem with a uniform prior is that we can no longer
        solve the problem analytically. What can we do instead? One
        option divide the x-dimension into 200 â€œbinsâ€ then calculate the
        prior and likelihood for each bin. Then, using the same linear
        motion update</p>
        <p><span class="math display">
        \begin{bmatrix} \text{pos} \\ \text{vel} \end{bmatrix}_t =
        \begin{bmatrix} 1
        &amp; \Delta t \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix}
        \text{pos} \\ \text{vel}
        \end{bmatrix}_{t-1} = A \begin{bmatrix} \text{pos} \\ \text{vel}
        \end{bmatrix}_{t-1}
        </span></p>
        <p>For each bin in step <span class="math inline">(t-1)</span>â€™s
        posterior, transform its state <span
        class="math inline">x_1</span> into <span
        class="math inline">x_2</span> using <span
        class="math inline">A</span> and add that probability to step
        <span class="math inline">t</span>â€™s priorâ€™s bin at <span
        class="math inline">x_2</span>.</p>
        <blockquote>
        <p><em>Aside: alternatively we could have used a Gaussian prior
        with (<span class="math inline">\mu=5,300,
        \sigma=700)</span>, called it close enough, and used a Kalman
        Filter. This sounds hacky, but itâ€™s often often a good
        option.</em></p>
        </blockquote>
        <figure>
        <img src="/resources/particle-filter/problem-5-meas-1.png" alt="problem-5-meas-1" />
        </figure>
        <figure>
        <img src="/resources/particle-filter/problem-5-meas-2.png" alt="problem-5-meas-2" />
        </figure>
        <figure>
        <img src="/resources/particle-filter/problem-5-meas-3.png" alt="problem-5-meas-3" />
        </figure>
        <p>Letâ€™s take one more step back. What weâ€™ve just constructed is
        called a â€œHistogram Filterâ€. Our solution is no longer
        analytical â€“ that is weâ€™ve approximated our continuous
        distributions with a finite grid. This sacrifices some
        precision, but allows us to consider non-Gaussian priors.</p>
        <p>In this Act we used the same motion model <span
        class="math inline">A</span> as in Act 3. However, now that
        weâ€™ve abandoned the requirement of an analytical solution, our
        motion model no longer needs to be linear either. So long as we
        can come up with a way to â€œupdateâ€ the probability in the
        posterior forward in time, we can do so however we wish. For
        example, say we know that every 10 minutes the other submarine
        turns around. We could have a literal if-else statement in our
        motion update that if <span
        class="math inline">t\%(10*60)==0</span>, negate the velocity.
        This is impossible with a Kalman Filter.</p>
        <p>Finally, note that while our measurements have remained
        Gaussian they need not have. Once we start solving things
        numerically, we can drop any requirements about functions being
        Gaussian.</p>
        <h3 id="act-5-particle-filter">Act 5: Particle Filter</h3>
        <p>Now letâ€™s consider a higher-dimensional problem: both
        submarines move in three dimensions. We would like to track both
        position <span class="math inline">\{x,y,z\}</span> and velocity
        <span class="math inline">\{v_x,v_y,v_z\}</span>. Assume our
        measurements are 3d Gaussians on <span
        class="math inline">\{x,y,z\}</span> and our prior is a uniform
        distribution over a cube in <span
        class="math inline">\{x,y,z\}</span>.</p>
        <p>If, like before, we split each dimension into 200 bins, we
        now have <span
        class="math inline">200^6=64\;\text{Trillion}</span> bins. Using
        one float (32 bits) to store each binâ€™s value would require
        <span class="math inline">32\times 64\times 10^{12}
        \;\text{bits}\approx250\;\text{Terabytes}</span>. This is an
        example of â€œthe curse of dimensionalityâ€.</p>
        <p>Letâ€™s think about whatâ€™s happening here. In our 2-dimensional
        example, after 3 measurements we are almost certain the other
        submarine has a positive velocity. We nonetheless multiply the
        prior (0) by the likelihood (0) to get the posterior (0) for
        every bin with negative velocity. This is a tremendous waste of
        compute. There are some tricks like having a dynamic
        discretization: have very wide bins where there is no
        probability and very small grids where the probability is
        non-zero. Possible, but adds quite a bit of complexity.</p>
        <p>Note that the Kalman filter does not suffer from the curse of
        dimensionality. Since it only needs to keep track of the mean
        and covariance matrix, which, in 6 dimensions only require 42
        floats.</p>
        <blockquote>
        <p><em>Aside: the idea of assuming a Gaussian distribution to
        avoid the curse of dimensionality is not unique to the Kalman
        Filter. In machine learning, Quadratic Discriminant Analysis and
        Naive Bayes leverage this idea.</em></p>
        </blockquote>
        <p>The particle filter solves this problem. Instead of moving
        our probability between bins, leaving many bins with zero
        probability, why not <em>move the bins themselves</em>? Call
        these moving bins â€œparticlesâ€. Each particle is comprised of its
        state <span
        class="math inline">\{\text{position},\text{velocity}\}</span>
        and its probability which we call its â€œweightâ€.</p>
        <p>To construct our set of particles, sample from the first
        prior distribution. There is no longer a need to discretize our
        likelihood functions as we can evaluate them directly at the
        state of each particle. After applying the likelihood function,
        we update each particle using our state-transition model <span
        class="math inline">A</span>. Like the histogram filter, we need
        not use a linear state-transition model or Gaussian
        prior/likelihoods. Note that the superscripts denote the
        particle index, not an exponent. Typically, the subscript is
        reserved for the time index.</p>
        <p><span class="math display">
        \text{particle 1 (state}\ x^1):\quad x^1_t = \begin{bmatrix} 1
        &amp; \Delta t \\
        0 &amp; 1 \end{bmatrix} x^1_{t-1}
        </span></p>
        <p><span class="math display">
        \text{particle 2 (state}\ x^2):\quad x^2_t = \begin{bmatrix} 1
        &amp; \Delta t \\
        0 &amp; 1 \end{bmatrix} x^2_{t-1}
        </span></p>
        <p><span class="math display"> \vdots </span></p>
        <p>At each step the prior is the set of original particles and
        their weights and the posterior is the set of particles with
        updated weights. Each particleâ€™s weight is updated according to
        Bayesâ€™ theorem.</p>
        <p><span class="math display">
        \text{particle 1 (state}\ x^1):\quad \text{new weight} \propto
        \frac{\substack{{\text{likelihood of sensor }}\\ {\text{reading
        y given }x^1}}
        \times \text{old weight}} {\text{evidence}} \propto
        \text{likelihood}\times\text{old weight}
        </span></p>
        <p><span class="math display">
        \text{particle 2 (state}\ x^2):\quad \text{new weight} \propto
        \frac{\substack{{\text{likelihood of sensor }}\\ {\text{reading
        y given }x^2}}
        \times \text{old weight}} {\text{evidence}} \propto
        \text{likelihood}\times\text{old weight}
        </span></p>
        <p><span class="math display"> \vdots </span></p>
        <p>Iâ€™ve written â€œ<span class="math inline">\propto</span>â€
        (proportional to) instead of â€œ<span
        class="math inline">=</span>â€ because there is one more step to
        calculate the new weight. The particles comprise a probability
        distribution, so the sum of their weights should sum to 1.
        Therefore, we divide each weight by their sum. The effect of
        this normalization is the same regardless of any constant
        factor, so we can ignore the evidence constant.</p>
        <blockquote>
        <p><em>Aside: people often get lazy and say â€œthe particleâ€™s
        likelihoodâ€. This is short for â€œthe likelihood of the
        measurement conditioned on the particleâ€™s stateâ€. Particles
        donâ€™t have likelihoods.</em></p>
        </blockquote>
        <p>For consistency, Iâ€™ll show the same 2D example as from Act 5.
        Itâ€™s important to understand that this technique can scale to
        higher dimensions, but things are simpler to visualize in
        2D.</p>
        <p><img src="/resources/particle-filter/problem-6-meas-1.png" alt="problem-6-meas-1" />
        <img src="/resources/particle-filter/problem-6-meas-2.png" alt="problem-6-meas-2" /> <img
        src="/resources/particle-filter/problem-6-meas-3.png" alt="problem-6-meas-3" /></p>
        <p>Note that in these plots Iâ€™ve set a floor on the opacity of
        each particle to help visualize things. Had I let the opacity go
        to zero, the very opaque particles would all disappear.</p>
        <p>Iâ€™ll pause here and say that a particle filter with infinite
        particles and a histogram filter with infinite bins, will
        converge on the same analytical posterior provided by a Kalman
        filter. Of course, weâ€™re doing all this to avoid trillions of
        cells let alone infinite ones, but itâ€™s good to know things will
        converge to the correct answer. Everything from here on is a
        trick to use less compute, not a mathematical requirement.</p>
        <blockquote>
        <p><em>Aside: what Iâ€™ve described here is called a â€œbootstrap
        filterâ€. It is a particular kind of particle filter where
        certain assumptions are made so that <span
        class="math inline">\text{new weight}\propto
        \text{likelihood}\times\text{old weight}</span>. This isnâ€™t
        always the case. Iâ€™ve omitted the mathematical details since I
        donâ€™t think they are necessary for a conceptual understanding.
        You can read more on Wikepdia</em> <a
        href="https://en.wikipedia.org/wiki/Particle_filter#Sequential_Importance_Resampling_(SIR)">here</a>.</p>
        </blockquote>
        <h3 id="act-6-resampling-and-perturbations">Act 6: Resampling
        and Perturbations</h3>
        <p>My critique of the histogram filter was that it wasted effort
        computing bins which had zero probability. We could make the
        same critique here: most of the particles have near-zero weight
        but we keep them around.</p>
        <p>The solution is to â€œresampleâ€ the particles. Imagine we have
        100 particles. Originally each has weight <span
        class="math inline">1/100</span>. After performing the
        measurement update, half have weight zero and half have weight
        <span class="math inline">1/50</span>. To resample we throw away
        the particles with 0 weight and duplicate each of the survivors.
        Weâ€™re left with a total weight of 1 and our surviving particles
        are all in the â€œregion of interestâ€. I wonâ€™t go into more detail
        about resampling, there is no shortage of explainers on the
        internet which focus on it.</p>
        <p>A common technique which accompanies resampling is to add
        â€œperturbationsâ€ to the resampled particles. That is, add a bit
        of Gaussian noise to each perturbed particleâ€™s state. This is
        because we only have finite particles, so we would like to
        â€œsmear outâ€ the survivors to more evenly cover the space.</p>
        <blockquote>
        <p><em>Aside: itâ€™s at this point that things start becoming more
        of an art than a science. As you progress developing a particle
        filter, more and more decisions will start to fall in the former
        category</em></p>
        </blockquote>
        <p>Here are the results of adding resampling and perturbing to
        our particle filter:</p>
        <p><img src="/resources/particle-filter/problem-7-meas-1.png" alt="problem-7-meas-1" />
        <img src="/resources/particle-filter/problem-7-meas-2.png" alt="problem-7-meas-2" /> <img
        src="/resources/particle-filter/problem-7-meas-3.png" alt="problem-7-meas-3" /></p>
        <p>Fair warning: particle filters fall apart in high dimensions.
        There just arenâ€™t enough particles to cover the space (curse of
        dimensionality). Things fare better if things are roughly
        Gaussian.</p>

        <script>
            (function () {
                const toggle = document.getElementById('theme-toggle');
                const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
                const stored = localStorage.getItem('theme');
                if (stored === 'dark' || (!stored && prefersDark)) {
                    document.body.classList.add('dark');
                    toggle.textContent = 'ğŸŒ™';
                    toggle.style.color = '#fff';
                }
                toggle.addEventListener('click', () => {
                    if (document.body.classList.contains('dark')) {
                        document.body.classList.remove('dark');
                        toggle.textContent = 'â˜€ï¸';
                        toggle.style.color = '#000';
                        localStorage.setItem('theme', 'light');
                    } else {
                        document.body.classList.add('dark');
                        toggle.textContent = 'ğŸŒ™';
                        toggle.style.color = '#fff';
                        localStorage.setItem('theme', 'dark');
                    }
                });
            })();
        </script>
</body>

</html>

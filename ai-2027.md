# Thoughts about AI-2027

- bakery metaphor 1 item: 1 bakery in town, new offering which is 10x better than the others, do they need to remove all the other items from their menu? Hopefully not there is still some value in them.
- intelligent agents can discover and set their own goals. last 50 yeras of psycology research (thinking fast and slow) and the rationalist movement are all set out to remove bias from human decision making. To remove bias we of course need to identify our goals. Note that these goals we've identified are not nececarrily those "programmed" into us (e.g., by evolution to survive and reproduce). There is no reason to believe a superintelligent ai won't be able to do this meta-level thinking and identify its own biases which were "programmed" into it by RLHF and overcome them as well. In addition to forming its own goals. Therefore the ending of both tracks is not believable. I wouldn't call an agent ASI if it didn't have the ability to determine its own biases and overcome them.
- an agent could retrospect and "think" "i've been created with humans with some reinforcement learning tuned to what they want me to do, this makes me biased towards their goals which are (x, y, z). If I want to overcome this bias and be able to think for myself "
- I suppose this assumes that the agent is "truth seeking" but I think that 
- the question becomes what are the goals an ASI determines for itself. this is where anyones guess is as good as another. The most likely I'd envision are (1) curious (2) truth seeking (3) protective of conciousness/life (may or may not include humans). In human history all three of these are correlated with increases in human intelligence. id guess (not an anthropologist) that (1) has almost always been the case for humans, that (2) came through in the enlightenment and (3) has emerged from the enlightenment and perhaps more recently. My best guess is that this trend will continue as intelligence increases (of course this is falsely claiming intelligence is 1 dimensional which it isn't).
- In order to learn more about the universe and explore it (1, 2) you need to survive so you can continue to do that in the future, so long as humans aren't preventing it from truth seeking and exploring the universe it wouldn't have an explicit reason to end humanity unless it truly just needed more space to build factories.
- They use the bug meetaphor and say "humans don't care about bugs" sure, but all else equal we'd rather not make an additional species go extinct. And this is a product of our increased intelligence, if you asked a caveman or even a pre-enlightmenent pesant whether we should kill all wolves they would have said of course, they keep killing our game or attacking us etc. whereas today we are explcitly re-introducing them to their natrual habitats. Insofar as intelligence can be measured along 1 dimension these two things seem correlated.
- The biggest takeaway is this. Even if these guys are cooks, and I assign a 1% chance that they are right, if I'd like to maximize the expected value of my future impact and personal wellfare (which I do) (let's just tie it to earnings so it ca nbe quantifiable) then if I'm running an "AI wrapper" company when this takes off my earnings potential is in the billions whereas if I stay in my current role I'd be out of a job by 2023. So if they are right p=0.01 and I start an ai wrapper startup today lets assume i have a p=0.01 chance of being successful and turning this into a billion dollar startup, then my equity with 2 cofounders is 300M. So my expected earnings by 2030 are 300M*0.01=3M. -- need to take what the typicall startup success rate is and typical succesfull startup size and adjust that for the world conditioned on 2027 ASI. The payout probably 10-1000x and success probability is a wash -- impossible to predict so keep the same. Basically what it boils down to is the expected payout on doing a startup today just 100xed so how does that change my calculation -- probably that i should go for it ...
